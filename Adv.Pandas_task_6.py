# -*- coding: utf-8 -*-
"""Copy of task-22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16yKo8SwuWW4gb_cv-MSzb7L5B21bWBKo
"""

import pandas as pd
import numpy as np
import re

pd.options.display.max_columns = None

"""### `Question-1:`

The dataset is based on NFTs. This is quite large. So, when you will read the dataset, you have to wait. The link of the dataset: https://tinyurl.com/2pfhncqw

Your task is to make a pivote table by using the columns "verification_status", "contract_type", "rarity_score" and "last_sale_price" to find out the total values.
"""

df = pd.read_csv('https://tinyurl.com/2pfhncqw')

df.pivot_table(index='verification_status', columns='contract_type', values=['rarity_score','last_sale_price'], aggfunc='sum')

"""### `Question-2:`

You are given a dataset about the cars' price and miles driven of different cars throughout the different years. The link of the dataset: https://tinyurl.com/2r24n45l. Your tasks are
- make a pivot table of the brands (the required brands are given below) from the "Year" 2018 to the year 2022 in which the "Price" is shown as average values and "Miles" are in median values. In this pivote table, every row represents a particular brand and each column represents either average "Price" of a partucular year or median values of "Miles" of a particular year.
- At the end plot a kde chart for the "Price" and "Miles" by using your pivot table.

There are some challenges to solve this task:
- There is no column that is represented to the brand name of the car. You have to find out on your own.
- Some values of "Years" column are misleading. You have to reset this column too that every value should tell a valid meaning.
"""

cars = pd.read_csv('https://tinyurl.com/2r24n45l')

cars['Brand'] = cars['Name'].str.split(' ').str.get(0)

cars['Year'] = cars['Year'].apply(lambda x:int(re.findall(r"\d{4}", str(x))[0]))

data = cars[(cars['Year']>=2018) & (cars['Year']<=2022)]
piv_data = data.pivot_table(index='Brand',columns='Year',values=['Price', 'Miles'], aggfunc={'Price':'mean', 'Miles':'median'})
piv_data.head()

piv_data['Price'].plot(kind='kde')

piv_data['Miles'].plot(kind='kde')

"""### `Question-3:`

You are given a dataset of **Daily Power Generation in India** of regional wise of all Power Stations. Link of the dataset: https://tinyurl.com/2nq6kugt

Task
- In this dataset, there are many columns. Two of them are `Actual(MU)` and `Excess(+) / Shortfall (-)`. `Actual(MU)` represents the actual power generation. `Excess(+) / Shortfall (-)` tells is that generated power is excess or shortfall for that particular day of a power station. You have to find out what should be the actual power generation required for that day by the power stations that there would not be any excess or shorfall power. For the result, make a new column.
- Find out the month of the day and store it as a new column.
- Find out top 10 frequently appeared power stations in this dataframe.
- Next create a pivot table of which every row represents a power station and every column represents a month in a order. Like January, February, March...
- It is hard to get inside if you look through the only pivot table. So plot the pivot table.
"""

df = pd.read_csv('https://tinyurl.com/2nq6kugt')

df['Required Power Generation'] = df['Actual(MU)'] - df['Excess(+) / Shortfall (-)']

df['Month'] = pd.to_datetime(df['Dates']).dt.month_name()

top_10 = df['Power Station'].value_counts().head(10).index.to_list()

df.columns

df1 = df[df['Power Station'].isin(top_10)]
piv = df1.pivot_table(index='Power Station', columns='Month', values='Required Power Generation', aggfunc='sum')

piv.plot()

"""###`Q-4` You are given a file `question-answer.csv`. Your task is to make a dataframe from it with two columns - `question` and `answers`.

* Questions in the file start from `Q<number>`; E.g.  `Q1` denotes question no. `1`
* Answers starts from `Ans<number>`; Eg. `Ans1` denotes answer of question no `1`
* MAke sure you look at columns name carefully

CSV File - "https://drive.google.com/file/d/10rmV3XrVtzpDTtYZF3UtCdcU0ajBJjGY/view?usp=share_link"
"""

data = pd.read_csv("/content/question-answer.csv")

questions = list(data['2'][::2])
answers = list(data['2'][1::2])
df = pd.DataFrame({'Questions':questions, 'Answers':answers})
df['Questions'] = df['Questions'].str.split('-').str.get(1)
df['Answers'] = df['Answers'].str.split('-').str.get(1)
df

"""###`Q-5`: Print Question and answer of those questions which does not contains any question mark (`?`).

"""

df['Questions'].str.endswith('?')

"""###`Q 6-10` LOG and EMPLOYEE
6. Show `activity` details month wise. Show count for each `activity`
7. Find employee who did most `activity` in January month.
8. Employee who have worked most no of times on Weekends.
9. Which activity is logged most on buisness days.
10. Week Days wise activity table.
```
log_file = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vT-OMrmmNdOTM-B5f5F1EpCutMVG230UZiLvqlsg0NIKUKR3yrqiI2r1pEX-LvSEk-3WwySPYtvbBC-/pub?gid=1937029224&single=true&output=csv")
employee = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vQ5AuFqRjSZVBKm5zyDxquX6utubq1DJKkYDI70vjeidAnyAu70KMSYpMYzeVSNVTeUIJBpfF6jU5E6/pub?gid=798824749&single=true&output=csv")

```

Note(for common field):- Employee file has `EMPLOYEE_ID` and LOG file has `emp_id`
"""

log_file = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vT-OMrmmNdOTM-B5f5F1EpCutMVG230UZiLvqlsg0NIKUKR3yrqiI2r1pEX-LvSEk-3WwySPYtvbBC-/pub?gid=1937029224&single=true&output=csv")
employee = pd.read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vQ5AuFqRjSZVBKm5zyDxquX6utubq1DJKkYDI70vjeidAnyAu70KMSYpMYzeVSNVTeUIJBpfF6jU5E6/pub?gid=798824749&single=true&output=csv")

log_file.rename({'emp_id':'EMPLOYEE_ID'},axis=1,inplace=True)

data = employee.merge(log_file, on='EMPLOYEE_ID')

data['dt'] = pd.to_datetime(data['dt'])

data['month'] = data['dt'].dt.month_name()

data.groupby(['activity','month'])['activity'].count()

data[data['month']=='January'].groupby(['EMPLOYEE_ID','FIRST_NAME','LAST_NAME'])['EMPLOYEE_ID'].value_counts().sort_values(ascending=False).head(1)

data.head()

#Employee who have worked most no of times on Weekends.
data[data['dt'].dt.day_of_week.isin([5,6])][['FIRST_NAME','LAST_NAME']].value_counts().head(1)

#Which activity is logged most on buisness days.
data[~data['dt'].dt.day_of_week.isin([5,6])]['activity'].value_counts().head(1)
#Week Days wise activity table

#Week Days wise activity table
data['day'] = data['dt'].dt.day_name()
data.pivot_table(index='day',columns='activity',aggfunc='count')['Log_ID']

"""### `Q-11`:

1. There are missing value in first name of employee. Fill it using email and last name field.

> E.g- `email` -> JMURMAN and `last_name` -> 'Urman' -> so make it's first name as 'JM', Sort of (Email- lastname).

> Email is constructed from initials of first name concate with lastname. Your Task is to fill first name initials in missing data.

2. You can see email field don't have any domain name. Change this to full email addreess with domain as 'campusx.com'.
E.g- Email field -> `JMURMAN`  result -> `JMURMAN@CAMPUSX.COM`

3. Show Full Name of all the employees whose name starts with 'A' and has done any of these activity ['Incpection', 'Cleaning', 'Checking]
"""

employee['FIRST_NAME'].fillna(employee['EMAIL'].str.split('').str.get(1), inplace=True)

employee['EMAIL'] = employee['EMAIL']+'@CAMPUSX.COM'

employee.info()

data = employee.merge(log_file, on='EMPLOYEE_ID')
x = data[data['activity'].isin(['Inspection', 'Cleaning', 'Checking'])]
x[x['FIRST_NAME'].str.startswith('A')]['FIRST_NAME'].unique()

